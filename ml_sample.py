# -*- coding: utf-8 -*-
"""ML-sample.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11KT-F8rPzDDrLk2-QSIsdAeKXFpKzniX

Various ML models to determine High School Grades
"""

# import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

filepath = '/content/drive/MyDrive/student-mat.csv'
df = pd.read_csv(filepath, delimiter=';')

df.describe

features = ['school',	'sex',	'age',	'address',	'famsize',
       'Pstatus',	'Medu',	'Fedu',	'Mjob',	'Fjob',	'reason',	'guardian',
       'traveltime'	, 'studytime',	'failures',	'schoolsup',	'famsup',	'paid',	
       'activities',	'nursery',	'higher',	'internet',	'romantic',	'famrel',	
       'freetime',	'goout',	'Dalc',	'Walc',	'health',	'absences']
X = df[features]
y = df['G3']

cat_features = ['school', 'sex', 'address', 'famsize', 
                'Pstatus', 'Mjob', 'Fjob', 'reason', 'guardian', 
                'schoolsup', 'famsup', 'paid', 'activities',
                'nursery', 'higher', 'internet', 'romantic']

# label encoding for categorical variables
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

for feature in cat_features:
  X[feature] = le.fit_transform(df[feature].astype(str))



from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.4)

"""1. Linear Regression"""

from sklearn.linear_model import LinearRegression
model1 = LinearRegression()
model1.fit(X_train, y_train)
y_pred1 = model1.predict(X_test)

from sklearn.metrics import r2_score, mean_absolute_error
print(r2_score(y_test, y_pred1))
print(mean_absolute_error(y_pred1, y_test))

"""Decision Tree"""

from sklearn.tree import DecisionTreeRegressor
model2 = DecisionTreeRegressor()
model2.fit(X_train, y_train)
y_pred2 = model2.predict(X_test)

from sklearn.metrics import r2_score, mean_absolute_error
print(r2_score(y_pred2, y_test))
print(mean_absolute_error(y_pred2, y_test))

# function for optimal amount of leaves
def get_mae(max_leaf_nodes, X_train, X_test, y_train, y_test):
  model3 = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)
  model3.fit(X_train, y_train)
  y_pred3 = model3.predict(X_test)
  mae = mean_absolute_error(y_test, y_pred3)
  return(mae)

candidates_leaves = [5, 25, 50, 100, 250, 500, 750, 800, 900]
for i in candidates_leaves:
  my_mae = get_mae(i, X_train, X_test, y_train, y_test)
  print(i, my_mae)

"""Random Forest"""

from sklearn.ensemble import RandomForestRegressor
model4 = RandomForestRegressor(n_estimators = 500)
model4.fit(X_train, y_train)
y_pred4 = model4.predict(X_test)

from sklearn.metrics import r2_score, mean_absolute_error
print(r2_score(y_test, y_pred4))
print(mean_absolute_error(y_pred4, y_test))

# write function for optimal amount of estimators
def get_mae(estimators, X_train, X_test, y_train, y_test):
  model5 = RandomForestRegressor(n_estimators=estimators, random_state=0)
  model5.fit(X_train, y_train)
  y_pred5 = model5.predict(X_test)
  mae = mean_absolute_error(y_test, y_pred5)
  return(mae)

candidates_estimators = [5, 25, 50, 100, 250, 500, 750, 800, 850] # 250 is best
for i in candidates_estimators:
  my_mae = get_mae(i, X_train, X_test, y_train, y_test)
  print(i, my_mae)

# we use 250 estimators
model5 = RandomForestRegressor(n_estimators=250, random_state=0)
model5.fit(X_train, y_train)
y_pred5 = model5.predict(X_test)


# permutation importance
import eli5
from eli5.sklearn import PermutationImportance

perm = PermutationImportance(model5, random_state=1).fit(X_test, y_test)
eli5.show_weights(perm, feature_names=X_test.columns.tolist())

"""Gradient Boosting"""

from xgboost import XGBRegressor
model6 = XGBRegressor(n_estimators = 500, learning_rate = 0.05)
model6.fit(X_train, y_train)

y_pred6 = model6.predict(X_test)
mae = mean_absolute_error(y_test, y_pred6)
print(mae)