# -*- coding: utf-8 -*-
"""thesis_notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/114ih_w98Fl5RhFgc_DkG9UaZEaavDUsM
"""
# test
# importing the libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score

import statsmodels.api as sm

! pip install dataprep

from dataprep.eda import plot, plot_correlation, create_report
from dataprep.eda.missing import plot_missing

"""Loading the datasets"""

# accepted loans
filepath1 = "/content/drive/MyDrive/Master Thesis/Lending Club Loan Data/accepted_sample_new.csv"

df1 = pd.read_csv(filepath1)

# denied loans
filepath2 = "/content/drive/MyDrive/Master Thesis/Lending Club Loan Data/denied_sample_new.csv"

df2 = pd.read_csv(filepath2)

df1.head()

# descriptive statitistics
report1 = create_report(df1, title='My Report 1')

df2.shape

"""Data cleaning for the accepted dataset"""

# create average fico score
df1['fico_score'] = (df1['fico_range_low'] + df1['fico_range_high']) / 2

# create categorical variable for credit grade
df1.grade = [ ord(x) - 64 for x in df1.grade ]

# create logs for income
df1['log_income'] = np.log(df1['annual_inc'])

# Correlations
from dataprep.eda import plot_correlation

cor_features = ['emp_length', 'annual_inc', 'dti', 'fico_score', 'GDP', 'Unemployment_rate', 'hc_coverage', 'education', 'crime_rate']
df_cor = df1[cor_features]
plot_correlation(df_cor)

# demographic statistics
# accepted dataset
x = df1['crime_rate'].mean()
print(x)
x2 = df1['crime_rate'].min()
print(x2)
x3 = df1['crime_rate'].max()
print(x3)

"""Scaling the demographics for accepted dataset"""

# Scale the demographic variables
scaler = StandardScaler()
features = ['GDP', 'Unemployment_rate', 'hc_coverage', 'education', 'crime_rate']

# the fit_transform ops returns a 2d numpy.array, we cast it to a pd.DataFrame
standardized_features = pd.DataFrame(scaler.fit_transform(df1[features].copy()), columns = features)
old_shape = df1.shape

# drop the unnormalized features from the dataframe
df1.drop(features, axis = 1, inplace = True)

# join back the normalized features
df_acc = pd.concat([df1, standardized_features], axis= 1)
assert old_shape == df_acc.shape, "something went wrong!"

# drop the blanks
df_acc.dropna(axis=0, how='all', inplace = True)

# Scale the demographic variables
scaler = StandardScaler()
features = ['GDP', 'Unemployment_rate', 'hc_coverage', 'education', 'crime_rate']

# the fit_transform ops returns a 2d numpy.array, we cast it to a pd.DataFrame
standardized_features = pd.DataFrame(scaler.fit_transform(df1[features].copy()), columns = features)
old_shape = df1.shape

# drop the unnormalized features from the dataframe
df1.drop(features, axis = 1, inplace = True)

# join back the normalized features
df_acc = pd.concat([df1, standardized_features], axis= 1)
assert old_shape == df_acc.shape, "something went wrong!"

# drop the blanks
df_acc.dropna(axis=0, how='all', inplace = True)

# add scaled variables together
df_acc['sum_factors'] = df_acc['GDP'] + df_acc['education'] + df_acc['hc_coverage'] - df_acc['Unemployment_rate'] - df_acc['crime_rate']

df_acc.shape

"""Creating groups for the accepted dataset"""

# we have to create seperate groups for each years and then add them back together
df_acc2008 = df_acc[df_acc['Year'] == 2008]
df_acc2009 = df_acc[df_acc['Year'] == 2009]
df_acc2010 = df_acc[df_acc['Year'] == 2010]
df_acc2011 = df_acc[df_acc['Year'] == 2011]
df_acc2012 = df_acc[df_acc['Year'] == 2012]
df_acc2013 = df_acc[df_acc['Year'] == 2013]
df_acc2014 = df_acc[df_acc['Year'] == 2014]
df_acc2015 = df_acc[df_acc['Year'] == 2015]

# check shape
df_acc2008.shape

# creating three groups for 2008
bin_labels_3 = ['1', '2', '3']
df_acc2008['group'] = pd.qcut(df_acc2008['sum_factors'], q=3, labels=bin_labels_3)

# creating three groups for 2009
bin_labels_3 = ['1', '2', '3']
df_acc2009['group'] = pd.qcut(df_acc2009['sum_factors'], q=3, labels=bin_labels_3)

# creating three groups for 2010
bin_labels_3 = ['1', '2', '3']
df_acc2010['group'] = pd.qcut(df_acc2010['sum_factors'], q=3, labels=bin_labels_3)

# creating three groups for 2011
bin_labels_3 = ['1', '2', '3']
df_acc2011['group'] = pd.qcut(df_acc2011['sum_factors'], q=3, labels=bin_labels_3)

# creating three groups for 2012
bin_labels_3 = ['1', '2', '3']
df_acc2012['group'] = pd.qcut(df_acc2012['sum_factors'], q=3, labels=bin_labels_3)

# creating three groups for 2013
bin_labels_3 = ['1', '2', '3']
df_acc2013['group'] = pd.qcut(df_acc2013['sum_factors'], q=3, labels=bin_labels_3)

# creating three groups for 2014
bin_labels_3 = ['1', '2', '3']
df_acc2014['group'] = pd.qcut(df_acc2014['sum_factors'], q=3, labels=bin_labels_3)

# creating three groups for 2015
bin_labels_3 = ['1', '2', '3']
df_acc2015['group'] = pd.qcut(df_acc2015['sum_factors'], q=3, labels=bin_labels_3)

# check shape of each dataset
df_acc2008.shape

# check to see if it has gone well
df_acc2008.head()

# add them back together
frames = [df_acc2008, df_acc2009, df_acc2010, df_acc2011, df_acc2012, df_acc2013,
          df_acc2014, df_acc2015]

df_acc_new = pd.concat(frames)

# check if it has the same shape as before
df_acc_new.shape

"""Scaling demographics for denied dataset"""

df2.head()

# create logs for amount requested
df2['log_amount'] = np.log(df2['Amount Requested'])

# Scale the demographic variables
scaler = StandardScaler()
features = ['GDP', 'unemp_rate', 'hc_coverage', 'education', 'crime_rate']

# the fit_transform ops returns a 2d numpy.array, we cast it to a pd.DataFrame
standardized_features = pd.DataFrame(scaler.fit_transform(df2[features].copy()), columns = features)
old_shape = df2.shape

# drop the unnormalized features from the dataframe
df2.drop(features, axis = 1, inplace = True)

# join back the normalized features
df_denied = pd.concat([df2, standardized_features], axis= 1)
assert old_shape == df_denied.shape, "something went wrong!"

# drop the blanks
df_denied.dropna(axis=0, how='all', inplace = True)

df_denied.head()

# add scaled variables together
df_denied['sum_factors'] = df_denied['GDP'] + df_denied['education'] + df_denied['hc_coverage'] - df_denied['unemp_rate'] - df_denied['crime_rate']

# check shape
df_denied.shape

"""Creating groups for the denied dataset"""

# we have to create seperate groups for each years and then add them back together
df_denied2008 = df_denied[df_denied['Year'] == 2008]
df_denied2009 = df_denied[df_denied['Year'] == 2009]
df_denied2010 = df_denied[df_denied['Year'] == 2010]
df_denied2011 = df_denied[df_denied['Year'] == 2011]
df_denied2012 = df_denied[df_denied['Year'] == 2012]
df_denied2013 = df_denied[df_denied['Year'] == 2013]
df_denied2014 = df_denied[df_denied['Year'] == 2014]
df_denied2015 = df_denied[df_denied['Year'] == 2015]

# creating three groups for 2008
bin_labels_3 = ['1', '2', '3']
df_denied2008['group'] = pd.qcut(df_denied2008['sum_factors'], q=3, labels=bin_labels_3)

# creating three groups for 2009
bin_labels_3 = ['1', '2', '3']
df_denied2009['group'] = pd.qcut(df_denied2009['sum_factors'], q=3, labels=bin_labels_3)

# creating three groups for 2010
bin_labels_3 = ['1', '2', '3']
df_denied2010['group'] = pd.qcut(df_denied2010['sum_factors'], q=3, labels=bin_labels_3)

# creating three groups for 2011
bin_labels_3 = ['1', '2', '3']
df_denied2011['group'] = pd.qcut(df_denied2011['sum_factors'], q=3, labels=bin_labels_3)

# creating three groups for 2012
bin_labels_3 = ['1', '2', '3']
df_denied2012['group'] = pd.qcut(df_denied2012['sum_factors'], q=3, labels=bin_labels_3)

# creating three groups for 2013
bin_labels_3 = ['1', '2', '3']
df_denied2013['group'] = pd.qcut(df_denied2013['sum_factors'], q=3, labels=bin_labels_3)

# creating three groups for 2014
bin_labels_3 = ['1', '2', '3']
df_denied2014['group'] = pd.qcut(df_denied2014['sum_factors'], q=3, labels=bin_labels_3)

# creating three groups for 2015
bin_labels_3 = ['1', '2', '3']
df_denied2015['group'] = pd.qcut(df_denied2015['sum_factors'], q=3, labels=bin_labels_3)

# check to see if it has gone well
df_denied2008.head()

# add them back together
frames = [df_denied2008, df_denied2009, df_denied2010, df_denied2011, df_denied2012, df_denied2013,
          df_denied2014, df_denied2015]

df_denied_new = pd.concat(frames)

# check if it has the same shape as before
df_denied_new.shape

"""Merging part of accepted loans and denied loans """

# making a df of parts of accepted df
dataframe = df_acc_new[['Year', 'State', 'loan_amnt', 'emp_length', 'dti', 'fico_score', 'group']]
dataframe.rename(columns={"loan_amnt": "Amount Requested"}, inplace=True)

# reindex to same order as denied
dataframe2 = dataframe.reindex(['Amount Requested','Year', 'State',	'fico_score',	'dti',
                                'emp_length',	'group'], axis=1)

# add dummy where accepted is 0 and denied is 1
dataframe2['Denied'] = 0

# denied df
df_denied_new.rename(columns={"Employment": "emp_length"}, inplace=True)
df_denied_new['Denied'] = 1

# add them together
df_merged = dataframe2.append(df_denied_new)

# remove rows where year is blank
df_merged = df_merged[df_merged['Year'].notna()]

df_merged.to_csv('df_merged_finall2.csv')

df_merged.head()

# create logs for amount requested
df_merged['log_amount'] = np.log(df_merged['Amount Requested'])

# split merged set on tiers
df_merged1 = df_merged[df_merged['group'] =='1']
df_merged2 = df_merged[df_merged['group'] =='2']
df_merged3 = df_merged[df_merged['group'] =='3']

df_merged2.shape

# check statistics per variable
plot(df_merged, 'Denied')

"""Statistics for Interest rate and credit grade for accepted loans"""

df_acc_new.head()

# keep rows where loan status is either paid off or charged off 
df_acc1 = df_acc_new[(df_acc_new.loan_status!= 'Current' ) & (df_acc_new.loan_status!= 'Does not meet the credit policy. Status:Charged Off') & (df_acc_new.loan_status!= 'Late (16-30 days)') &
    (df_acc_new.loan_status!= 'In Grace Period') & (df_acc_new.loan_status!= 'Does not meet the credit policy. Status:Fully Paid') & (df_acc_new.loan_status!= 'Late (31-120 days)')]

# change loan status: default or charged off is 1, fully paid is 0
df_acc1['loan_status'].replace({'Charged Off': 1, 'Default': 1, 'Fully Paid': 0}, inplace = True)

# split set on tiers (for statistics)
df_acc11 = df_acc1[df_acc1['group'] =='1']
df_acc22 = df_acc1[df_acc1['group'] =='2']
df_acc33 = df_acc1[df_acc1['group'] =='3']

# anova tests for state-level factors
import scipy.stats as stats

fvalue, pvalue = stats.f_oneway(df_acc1[''], df['B'], df['C'], df['D'])

df_acc11.head()

"""Defaults"""

# check for nan values
null =df_acc33[df_acc33.isnull().any(axis=1)]
null

# check means
plot(df_acc33, 'rate_of_return')

df_acc33.shape

"""Regressions"""

df_acc1['group'].describe()

# create dummies for group variable
dummies = pd.get_dummies(df_acc1[['group']])
df_acc2 = pd.concat([df_acc1, dummies], axis=1)

df_acc2.head()

# change group to integer
df_acc2['group'] = df_acc2['group'].astype(int)

# create logs for amount
df_acc2['log_funded_amount'] = np.log(df_acc2['funded_amnt'])

df_acc2.to_csv('df_acc2.csv')

"""Dependent variable: interest rate"""

# interest rate as dependent variable
X = df_acc2[['emp_length', 'log_income', 'dti', 'fico_score', 'group_1', 'group_2']]
y = df_acc2['int_rate']

linear_model=sm.OLS(y,X)
result=linear_model.fit()
result.summary()

# odds
OR=np.exp(result.params)
OR

"""Dependent variable: funded amount"""

# funded amount as dependent variable
X = df_acc2[['emp_length', 'log_income', 'dti', 'fico_score', 'group_1', 'group_2']]
y = df_acc2['log_funded_amount']

linear_model=sm.OLS(y,X)
result=linear_model.fit()
result.summary()

"""Dependent variable: Default"""

# default as dependent variable

X = df_acc2[['emp_length', 'log_income', 'dti', 'fico_score', 'group_1', 'group_2']]
y = df_acc2['loan_status']

log_model=sm.Logit(y,X)
result=log_model.fit()
result.summary()

# odds
OR=np.exp(result.params)
OR

# rate of return as dependent variable
#X = df_acc2[['emp_length', 'log_income', 'dti', 'fico_score', 'group_1', 'group_2', 'group_3']]
#y = df_acc2['rate_of_return']

#linear_model=sm.OLS(y,X)
#result=linear_model.fit()
#result.summary()

# rate of return as dependent variable with interactions
#X = df_acc2[['emp_length', 'annual_inc', 'dti', 'fico_score', 'group_1', 'group_2', 'group_3', 
             #'emp_length*group', 'dti*group',	'fico_score*group',	'annual_inc*group' ]]
#y = df_acc2['rate_of_return']

#linear_model=sm.OLS(y,X)
#result=linear_model.fit()
#result.summary()

"""Merged set"""

df_merged = pd.read_csv('/content/df_merged_finall.csv')
df_merged.head()

# create dummies for group variable
df_merged['group'] = df_merged['group'].astype('str')

dummies = pd.get_dummies(df_merged[['group']])
df_merged2 = pd.concat([df_merged, dummies], axis=1)

df_merged2['group'] = df_merged2['group'].astype(int)

df_merged2.head()

# group 1
df_merged2['emp_length*group_1'] = df_merged2['emp_length']*df_merged2['group_1']
df_merged2['dti*group_1'] = df_merged2['dti']*df_merged2['group_1']
df_merged2['fico_score*group_1'] = df_merged2['fico_score']*df_merged2['group_1']
df_merged2['log_amount*group_1'] = df_merged2['log_amount']*df_merged2['group_1']

# group 2
df_merged2['emp_length*group_2'] = df_merged2['emp_length']*df_merged2['group_2']
df_merged2['dti*group_2'] = df_merged2['dti']*df_merged2['group_2']
df_merged2['fico_score*group_2'] = df_merged2['fico_score']*df_merged2['group_2']
df_merged2['log_amount*group_2'] = df_merged2['log_amount']*df_merged2['group_2']

# group 3
df_merged2['emp_length*group_3'] = df_merged2['emp_length']*df_merged2['group_3']
df_merged2['dti*group_3'] = df_merged2['dti']*df_merged2['group_3']
df_merged2['fico_score*group_3'] = df_merged2['fico_score']*df_merged2['group_3']
df_merged2['log_amount*group_3'] = df_merged2['log_amount']*df_merged2['group_3']

# denied as dependent variable
# group 1
X = df_merged2[['log_amount', 'fico_score', 'dti', 'emp_length', 'group_1', 'group_2']]
y = df_merged2['Denied']

log_model=sm.Logit(y,X)
result=log_model.fit()
result.summary()

# odds
OR2=np.exp(result.params)
OR2